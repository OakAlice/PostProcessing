# Methods
### Data
#### Sourcing
Data for this study was raw animal-borne accelerometer data, including sampling timestamps, ID of sampled individual, and unprocessed tri-axial accelerometer recordings. Data was acquired from 15 sources, capturing raw accelerometer movement data from 13 species. Datasets sources and characteristics are described in Table 1 below. Data included labelled and unlabelled sources. Labelled data contained ground-truthed behavioural labels and was used as the training dataset; all included datasets included training data. Unlabelled data lacked ground-truthed behavioural annotation and was generally from deployment on wild free-roaming animals. This data was used to address ecological questions and was only sourced from a minority of papers.

While the Biologger Ethogram Benchmark (BEBE; Hoffman et al., 2024) is a pre-existing collection of collated animal-borne accelerometer datasets intended to be used for comparative classification tasks exactly as undertaken in this study, the format of the data included in the BEBE does not include the timestamp of the behavioural event. As this present study required identification of continuous vs non-continuous sequences, non-timestamped data could not be used, and thus the data had to be collected from primary sources. However, it should be noted that several of the datasets used in this study are also included in the BEBE and attention should be brought to the BEBE as resource for other research questions.

***Table 1.** Sources and characteristics of each of the raw animal-borne accelerometer datasets included in the study* ==Not all datasets are finished processing==

| Dataset                | Species                     | Citation                    | Source   | Available Unlabelled | Number individuals | Number behaviours |
| ---------------------- | --------------------------- | --------------------------- | -------- | -------------------- | ------------------ | ----------------- |
| Annett_Possum          | Brushtail Possum            | Annett et al., 2024         | Personal | Yes                  |                    |                   |
| Clemente_Echidna       | Long-beaked Echidna         | Clemente et al., 2016       | Personal | Yes                  |                    |                   |
| Dunford_Cat            | Domestic Cat (mixed breeds) |                             | link     |                      |                    |                   |
| Ferdinandy_Dog         | Domestic Dog (mixed breeds) | Ferdinandy et al., 2020     | link     |                      |                    |                   |
| Gaschk_Quoll           | Northern quoll              | Gaschk et al., 2023         | Personal | Yes                  |                    |                   |
| HarveyCarroll_Pangolin | African Pangolin            | Harvey-Carroll et al., 2016 | link     |                      |                    |                   |
| Ladds_Seal             |                             | Ladds et al., 2017          | link     |                      |                    |                   |
| Maekawa_Gull           |                             |                             | link     |                      |                    |                   |
| Mauny_Goat             | Domestic Goat               |                             | link     |                      |                    |                   |
| Smit_Cat               | Domestic Cat (mixed breeds) |                             | link     |                      |                    |                   |
| Sparkes_Koala          | Koala                       | Sparkes et al., unpublished | personal | Yes                  |                    |                   |
| Studd_Squirrel         | Red Squirrel                | Studd et al., 2021          | link     |                      |                    |                   |
| Pagano_Bear            |                             | Pagano et al., 2017         | link     |                      |                    |                   |
| Vehkaoja_Dog           | Domestic Dog (mixed breeds) |                             | link     |                      |                    |                   |
| Yu_Duck                | Pacific Black Duck          | Yu et al., 2021             | link     |                      |                    |                   |
#### Assessing Continuosness
For each dataset, metrics of the 'continuousness' of records in the training data was collected --- i.e., how much of the data was collected in continuous sequence, containing natural transitions between behaviours, vs. non-sequential behavioural snippets. To calculate this, all continuous sequences of data were extracted and the number of transitions (changes in behavioural label) present in the data was recorded. For each dataset, the following metrics were calculated:
* Proportion including transitions: Of all continuous sequences, how many contained behavioural transitions. Expressed as decimal percentage.
* Average transitions per sequence: Across all continuous sequences, what was the average number of behavioural transitions recorded. Expressed as numeric count.
### Producing Predictions
#### Feature Generation
All raw data was processed into features. All datasets were processed by defining a window length (1 second for all except Studd_Squirrel, which had a sampling frequency of 1 second and was therefore processed to 5 second windows, and X, which had a sampling rate of 5Hz and was processed to XX second windows to ensure sufficient samples to generate time-series features). Each window had a 10% overlap with the preceding window to ensure the capture of transition events and boost number of samples. 

For each window, a total of 203 features were generated, including the features specified in [[Tatler et al., 2016]] and all timeseries features from the R package 'tsfeatures'. For full feature list, see linked github code repository: [[will link here]]. Time was set to the first time for that window. In the labelled data, "Activity" was set to the most common label for that window. No Activity was assigned in the unlabelled data. This processed feature data was used for subsequent 
#### Hold-out Test Set
A hold-out test set was extracted from all datasets. In all but one case, test data was made up of the full data from a randomly selected 30% of individuals from that dataset. This method of testing on the full dataset of individuals not included in the training dataset is known to be the most robust against model overfitting (Ferdinandy et al., 2020; Wilson et al., 2025). In the case of Sparkes_Koala dataset, there was insufficient labelled data to split by individual and thus, data was split chronologically with the first 80% of chronologically collected data from each ID and Activity being selected as trainign data with the remaining 20% forming the test data. While this form of model testing does not test against overfitting to the specific labelled individuals, it is robust to over-fitting to the specific behavioural events (Aulsebrook et al., 2024; Wilson et al., 2025).
#### Model Tuning and Training
The remainder of the non-test data was used to tune and train the behavioural classification model. The base behavioural classification model architecture was the Random Forest from the 'ranger' package. Tuning involves adjusting the hyperpareters of the model (parameters controliing model shape and learning ability that are set prior to learning from the data) to customise it to the data and problem. The Random Forest model has hyperparameters for the number of trees in the forest (number.trees, trialled 10-1000), the maximum number of variables to be considered at each node (mtry, trialled 3-20), and the depth of the trees (max.depth, trialled 10-50). The optimal value of these hyperparameters was tuned using the 'rBayesianOptimisation' package. 

For each set of possible hyperparameters, the model was trained and tested three times using individual-based bootstrapping such that 80% of individual's datasets were used as training data and the remaining 20% were used as the validation set, but for each repetition the individuals in each set were randomised. Due to class imbalance, models were weighted by the prevalence of each class the training data. Performance was measured as the average weighted F1 score between the three replicates where weighted F1 score was measured as the average F1 score of all individual classes in the model, weighted by their prevalence in the validation data. The hyperparameter set with the highest overall F1 score was selected as the optimal model for that dataset (==Table S1 for Optimal Model Variables... will have a supplementary materials or appendix section==). These optimal hyperparameters were used to train a final model including all non-test data.
#### Model Prediction
This final model was then used to predict behavioural classes onto the hold-out test data. Each prediction was expressed as the probability that the event belonged to each of the possible classes, and the "activity" prediction was selected as the class with the greatest probability. For datasets where unlabelled data was available, the trained model was also applied to these datasets. Additionally, the models were applied back to the training data to provide a "training example" of error in the model (see below for further description).
### Post-Processing
#### Applying Smoothing Algorithms
To determine the effect of post-processing on the classification output of the models, seven potential post-processing methods were compared to the control (no post-processing) predictions. These methods were as follows in Table 2.

***Table 2.** Summarisation of seven post-processing methods trialled in this analysis.*

| Method         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Package |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- |
| None (Control) | No post-processing was applied.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |         |
| Mode           | A rolling window of five consecutive predictions was applied. The central prediction in each window was replaced with the modal (most frequent) behaviour within the rolling window.                                                                                                                                                                                                                                                                                                                                                               |         |
| Duration       | The 95th percentile of the minimum observed duration for each behaviour was calculated from the ground-truthed training data. In the test data, any predicted sequence shorter than this threshold was coerced to the preceding behaviour to eliminate implausibly brief events.                                                                                                                                                                                                                                                                   |         |
| Confusion      | A confusion matrix was derived from the predictions on the training set, finding the probability of misclassification between each pair of ground-truth vs predicted behaviours. These probabilities were used to inform a post-processing smoother. In the test data, at every predicted transition, if the likelihood of the new class being a misclassification of the preceding class exceeded 50% (according to the matrix learnt from the training data), the transition was denied and the prediction was reassigned to the previous class. |         |
| Transition     | A transition probability matrix was calculated from the training data, capturing the likelihood of one behaviour transitioning to another within each continuous recording sample. In the test data, transitions with less than 50% probability were considered biologically implausible and corrected by reverting the transition to the preceding behaviour.                                                                                                                                                                                     |         |
| HMM            | A Hidden Markov Model was trained on the training data with the ground-truthed labels used as the true states and the predictions as the emissions. This model was then applied to the test data predictions (emissions) to determine the "true states" as smoothed predictions within each continuous recording sample.                                                                                                                                                                                                                           | xxx     |
| Bayesian       | etc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | xxx     |
| LSTM           | etc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | xxx     |
Each method was applied to each species test data predictions to produce post-processing corrected "smoothed predictions". In datasets where unlabelled data was available, these predictions were also post-processed with each of the possible methods to produce smoothed predictions.
#### Performance of Smoothing Algorithms
For each post-processing method, performance was assessed in two ways. Firstly, performance was calculated in terms of improvement to classification performance by calculating both class-specific as well as macro-averaged Accuracy, Precision, Recall, and prevalence weighted F1-Score. The model with the greatest increase in weighted F1 compared to the control (no smoothing) was considered the optimal method.

Secondly, the impact of these post-processing changes on the overall ecological interpretations was assessed using the smoothed predictions on the unlabelled data, where available. Two simple ecological questions were devised for each dataset, and the same ecological analysis conducted on each of post-processed predictions to determine how the ecological outcome was affected by the smoothing that had been applied. As there are no ground-truthed labels for these data, it is not possible to determine which of the results is more accurate, and due to the subjectivity of the ecological interpretation, nor is it intrinsically evident which would be "better". Instead, case-by-case ecologist interpretation is required to make the final decision on the robustness and usefulness of the ecological results.

For each dataset, a sequence question was designed to calculate the average duration of a bout of a particular target behaviour where a 'bout' was defined as a continuous sequence of the same behaviour. Additionally, a proportion question was designed to calculate the total proportion of an individual's day spend in the target behaviour. In the each of the four datasets with available unlabelled data, the target behaviour selected was "Walking" The questions were therefore to determine the average duration of a walking bout, and the total proportion of the day spent walking.
#### Comparison of Methods
Finally, to determine which of the methods was optimal for post-processing accelerometer-based behavioural predictions, the difference between the weighted-average F1-score for each post-processing method and the score for the control method was calculated as the 'relative improvement'. A mixed effects model was constructed with relative improvement as the dependent variable and method, proportion of sequences including transitions, and average transitions per sequence as predictor variables, and species as a random effect.

==unsure whether Species should be a random effect or a variable? since they aren't true replicates of each other but I'm treating them like they are==
# Results

==Results aren't complete because some of the datasets are still processing but I've put in what I've got so far... presuming that it wont change drastically==
### Relative Performance Changes
The performance of the base classifier with no post-processing (control) differed between species with a range 0.2-0.7 weighted-average F1-score. Each post-processing method had a differing effect on each of the species. The unique characteristics of each dataset make their performance challenging to compare to each other, but considering only the relative differences between the post-processed performance and control method, some trends can be observed.

==***Unsure which metric to use so have shown 2. The results dont change, but slight details in the stats do so it doesn't really matter???***==
*first fig is (Score - Baseline) / Baseline*

![[Pasted image 20250702120659.png|700]]
*second fig is (Score - Baseline)*
![[Pasted image 20250702141035.png]]
***Figure 2.** Relative change in weighted-average F1-Score for each dataset and post-processing method. Star symbols represent the performance of the base-classifier with no post-processing (control) --- set to 0 as the baseline. Colours represent performance of each trialled post-processing method as a relative change from the performance of the control.*

Firstly, a linear mixed-effects model was used to assess the effects of post-processing method on the relative change in classification performance, with Species included as a random effect. Use of Naive Bayes post-processing was found to have a significant positive effect (β = 0.313, SE = 0.098, t(53) = 3.21, p = 0.002) as was duration-based post-processing (β = 0.259, SE = 0.101, t(53) = 2.56, p = 0.013). Other methods such as Mode-based, Transition, and HMM Post-processing showed marginal effects (p = 0.056–0.080) while Confusion and LSTM methods did not significantly change from the control.
#### Looking at the effect of continuousness
- To assess the effect of the continuousness of recording in the training data, these covariates (proportion of sequences that included transitions, and the rate of transitions) were added to the model. Firstly, the importance of the interactions between the three predictor variables was assessed by comparing three models: minimal with no interactions, medium with all 2-way interactions, and a full model with three-way interactions. The medium and full model were found to both be significantly better at explaining the variance in the model than the minimal model, and thus the full model was used. This full model found multiple significant effects.
- When accounting for transition rate and proportion of sequences with transitions, HMM and LSTM methods had a significantly negative effect on performance (_β_ = –0.445, _SE_ = 0.151, _t_ = –2.95, _p_ = 0.0045 and _β_ = –0.344, _SE_ = 0.150, _t_ = –2.29, _p_ = 0.0255, respectively). However, the performance of HMM improved significantly with increased transition rate (_β_ = 14.86, _p_ = 0.0063) and proportion of transitions (_β_ = 0.898, _p_ = 0.0033). There was also a three way interaction, though, with a negative effect (_β_ = –24.31, _SE_ = 6.80, _t_ = –3.57, _p_ = 0.0007).

However, Transition_Rate and Prop_Transitions have a weak covariation.. If I drop the latter and just use the former, then results are a little different. 
- For one, the medium model is the best explanatory model.
- Bayesian Smoothing had a significant positive effect on performance (β = 0.098, p = 0.049).
- A significant positive interaction between LSTM Smoothing and Transition Rate (β = 1.83, p = 0.0048)
- The interaction between Transition Smoothing and transition rate was marginally significant (β = 1.20, p = 0.058).
- This is slightly more what I would have expected seeing...

### Changes to Ecological Interpretation
==have only done this for 1 individual from 1 dataset just due to processing time and my desktop being busy with other stuff but its still proof of theory ==
![[Pasted image 20250619210043.png|700]]
# Discussion
==just some notes to show where my thinking is going==
In this study, we compared the effect of seven post-processing techniques on the classification performance of accelerometer-based behavioural classification predictions. 

**Base performance**
- The base, control, classification differed in performance between species between 0.2-0.7 weighted-average F1-score. 
- Studd_Squirrel dataset did the worst (likely because of lowest sample rate) and the Sparkes_Koala did the best (probably because it was validated differently to the remainder datasets and thus had a significant advantage). 
- Performance within the other datasets would have been dependent on various factors such as volume of data, number of classes (affecting random baseline), and the distinctiveness between the class boundaries --- variables that are all beyond the scope of this study.

**Effect of post-processing**
- Naive Bayes and Duration-based smoothing were both found to produce significant improvements in the classification performance (ranging from 0.01 to 0.4 improvement). Mode-based, Transition, and HMM had marginal effects close to significance. 
- This suggests that inclusion of a post-processing method will improve performance, but differently for each dataset. Worth playing around with your datasets strengths and weaknesses to see which of the post-processing methods are best for your data.

**Impact on training transitions on post-processing**
- When accounting for transition rate and proportion of sequences with transitions, HMM and LSTM methods had a significantly negative effect. HMM improved significantly with increased transition rate and proportion of transitions, but there was also a three way interaction with a negative effect suggesting that HMM is helpful in dynamic sequences with rapid transitions.
- However, when you just use the transition rate,  there's a significant positive interaction between LSTM Smoothing and transition rate, with a marginal effect of transition rate on Transition Smoothing.
- I find it very weird that Bayesian Smoothing isn't affected by the transition rate...?
- A limitation for all post-processing methods that learn from natural sequences in the training data however is that they rely on those natural sequences being represented in the training data. That is, these methods will only present improvements when the training data represents long continuous stretches of behaviour, containing multiple natural transitions. While some studies collect behaviour in this way, it is not true of all studies. In the koala data used in this study, for example, training data was not collected in sequence but as isolated representative snippets. More than 90% of sequences in the training data contained only 1 behaviour, with 2 behaviours in only very small minority. Thus, there were very few transitions for the models to learn from, providing such limited information that the higher-order post-processing methods failed to confer advantages over the most basic modal smoothing. Given that several of the models had interactions with the metrics of continuousness, it may explain why these models could not effectively learn from the data.

**Meaning of the study**
By incorporating higher-level inference, and utilising the sequential information available in time-series data, we are able to achieve higher classification performance without needing perfect classification accuracy during the ML prediction stage. In light of this performance gain, as a research community, we must now ask whether the substantial effort required to marginally improve classification model performance is still necessary, particularly given that simpler post-processing steps enable similar gains. Enhancing a model often demands significant resources such as collecting new data from rare behaviours or additional individuals, computationally expensive  hyper-parameter tuning, and careful balancing of model power against over-fitting. In contrast, post-processing methods can be readily applied to existing model outputs, using the data already available. 

Furthermore, in many ecological studies, the objective is not to capture behaviour at the finest temporal resolution, but to reveal broader patterns of activity. These coarse-scale trends are typically robust to local misclassifications and may not require high-fidelity predictions at every time step, so long as the outputs reflect meaningful ecological processes. Thus, given that perfection is neither possible nor necessary, a more parsimonious, easily implemented, and interpreted result is suggested in post-processing.

**Recommendations**
- In future, we should aim to collect more natural training data representative of true sequences of behaviour and including the transitions that we will be modelling out in reality.
- Design a perfect smoother by incorporating detailed ecological knowledge about plausible sequences and meaningful duration.
- This study is a first exploratory step into the possibility of incorporating ecological knowledge into the way we design and deploy these models. We hope to inspire others to consider the potential for this approach.