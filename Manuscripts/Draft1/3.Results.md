==Results aren't complete because some of the datasets are still processing but I've put in what I've got so far... presuming that it wont change drastically==
### Relative Performance Changes
The performance of the base classifier with no post-processing (control) differed between species with a range 0.2-0.7 weighted-average F1-score. Each post-processing method had a differing effect on each of the species. The unique characteristics of each dataset make their performance challenging to compare to each other, but considering only the relative differences between the post-processed performance and control method, some trends can be observed.

==***Unsure which metric to use so have shown 2. The results dont change, but slight details in the stats do so it doesn't really matter???***==
*first fig is (Score - Baseline) / Baseline*

![[Pasted image 20250702120659.png|700]]
*second fig is (Score - Baseline)*
![[Pasted image 20250702141035.png]]
***Figure 2.** Relative change in weighted-average F1-Score for each dataset and post-processing method. Star symbols represent the performance of the base-classifier with no post-processing (control) --- set to 0 as the baseline. Colours represent performance of each trialled post-processing method as a relative change from the performance of the control.*

Firstly, a linear mixed-effects model was used to assess the effects of post-processing method on the relative change in classification performance, with Species included as a random effect. Use of Naive Bayes post-processing was found to have a significant positive effect (β = 0.313, SE = 0.098, t(53) = 3.21, p = 0.002) as was duration-based post-processing (β = 0.259, SE = 0.101, t(53) = 2.56, p = 0.013). Other methods such as Mode-based, Transition, and HMM Post-processing showed marginal effects (p = 0.056–0.080) while Confusion and LSTM methods did not significantly change from the control.
#### Looking at the effect of continuousness
- To assess the effect of the continuousness of recording in the training data, these covariates (proportion of sequences that included transitions, and the rate of transitions) were added to the model. Firstly, the importance of the interactions between the three predictor variables was assessed by comparing three models: minimal with no interactions, medium with all 2-way interactions, and a full model with three-way interactions. The medium and full model were found to both be significantly better at explaining the variance in the model than the minimal model, and thus the full model was used. This full model found multiple significant effects.
- When accounting for transition rate and proportion of sequences with transitions, HMM and LSTM methods had a significantly negative effect on performance (_β_ = –0.445, _SE_ = 0.151, _t_ = –2.95, _p_ = 0.0045 and _β_ = –0.344, _SE_ = 0.150, _t_ = –2.29, _p_ = 0.0255, respectively). However, the performance of HMM improved significantly with increased transition rate (_β_ = 14.86, _p_ = 0.0063) and proportion of transitions (_β_ = 0.898, _p_ = 0.0033). There was also a three way interaction, though, with a negative effect (_β_ = –24.31, _SE_ = 6.80, _t_ = –3.57, _p_ = 0.0007).

However, Transition_Rate and Prop_Transitions have a weak covariation.. If I drop the latter and just use the former, then results are a little different. 
- For one, the medium model is the best explanatory model.
- Bayesian Smoothing had a significant positive effect on performance (β = 0.098, p = 0.049).
- A significant positive interaction between LSTM Smoothing and Transition Rate (β = 1.83, p = 0.0048)
- The interaction between Transition Smoothing and transition rate was marginally significant (β = 1.20, p = 0.058).
- This is slightly more what I would have expected seeing...

### Changes to Ecological Interpretation
==have only done this for 1 individual from 1 dataset just due to processing time and my desktop being busy with other stuff but its still proof of theory ==
![[Pasted image 20250619210043.png|700]]
