### Data
#### Sourcing
Data for this study was raw animal-borne accelerometer data, including sampling timestamps, ID of sampled individual, and unprocessed tri-axial accelerometer recordings. Data was acquired from 15 sources, capturing raw accelerometer movement data from 13 species. Datasets sources and characteristics are described in Table 1 below. Data included labelled and unlabelled sources. Labelled data contained ground-truthed behavioural labels and was used as the training dataset; all included datasets included training data. Unlabelled data lacked ground-truthed behavioural annotation and was generally from deployment on wild free-roaming animals. This data was used to address ecological questions and was only sourced from a minority of papers.

While the Biologger Ethogram Benchmark (BEBE; Hoffman et al., 2024) is a pre-existing collection of collated animal-borne accelerometer datasets intended to be used for comparative classification tasks exactly as undertaken in this study, the format of the data included in the BEBE does not include the timestamp of the behavioural event. As this present study required identification of continuous vs non-continuous sequences, non-timestamped data could not be used, and thus the data had to be collected from primary sources. However, it should be noted that several of the datasets used in this study are also included in the BEBE and attention should be brought to the BEBE as resource for other research questions.

***Table 1.** Sources and characteristics of each of the raw animal-borne accelerometer datasets included in the study* ==Not all datasets are finished processing==

| Dataset                | Species                     | Citation                    | Source   | Available Unlabelled | Number individuals | Number behaviours |
| ---------------------- | --------------------------- | --------------------------- | -------- | -------------------- | ------------------ | ----------------- |
| Annett_Possum          | Brushtail Possum            | Annett et al., 2024         | Personal | Yes                  |                    |                   |
| Clemente_Echidna       | Long-beaked Echidna         | Clemente et al., 2016       | Personal | Yes                  |                    |                   |
| Dunford_Cat            | Domestic Cat (mixed breeds) |                             | link     |                      |                    |                   |
| Ferdinandy_Dog         | Domestic Dog (mixed breeds) | Ferdinandy et al., 2020     | link     |                      |                    |                   |
| Gaschk_Quoll           | Northern quoll              | Gaschk et al., 2023         | Personal | Yes                  |                    |                   |
| HarveyCarroll_Pangolin | African Pangolin            | Harvey-Carroll et al., 2016 | link     |                      |                    |                   |
| Ladds_Seal             |                             | Ladds et al., 2017          | link     |                      |                    |                   |
| Maekawa_Gull           |                             |                             | link     |                      |                    |                   |
| Mauny_Goat             | Domestic Goat               |                             | link     |                      |                    |                   |
| Smit_Cat               | Domestic Cat (mixed breeds) |                             | link     |                      |                    |                   |
| Sparkes_Koala          | Koala                       | Sparkes et al., unpublished | personal | Yes                  |                    |                   |
| Studd_Squirrel         | Red Squirrel                | Studd et al., 2021          | link     |                      |                    |                   |
| Pagano_Bear            |                             | Pagano et al., 2017         | link     |                      |                    |                   |
| Vehkaoja_Dog           | Domestic Dog (mixed breeds) |                             | link     |                      |                    |                   |
| Yu_Duck                | Pacific Black Duck          | Yu et al., 2021             | link     |                      |                    |                   |
#### Assessing Continuosness
For each dataset, metrics of the 'continuousness' of records in the training data was collected --- i.e., how much of the data was collected in continuous sequence, containing natural transitions between behaviours, vs. non-sequential behavioural snippets. To calculate this, all continuous sequences of data were extracted and the number of transitions (changes in behavioural label) present in the data was recorded. For each dataset, the following metrics were calculated:
* Proportion including transitions: Of all continuous sequences, how many contained behavioural transitions. Expressed as decimal percentage.
* Average transitions per sequence: Across all continuous sequences, what was the average number of behavioural transitions recorded. Expressed as numeric count.
### Producing Predictions
#### Feature Generation
All raw data was processed into features. All datasets were processed by defining a window length (1 second for all except Studd_Squirrel, which had a sampling frequency of 1 second and was therefore processed to 5 second windows, and X, which had a sampling rate of 5Hz and was processed to XX second windows to ensure sufficient samples to generate time-series features). Each window had a 10% overlap with the preceding window to ensure the capture of transition events and boost number of samples. 

For each window, a total of 203 features were generated, including the features specified in [[Tatler et al., 2016]] and all timeseries features from the R package 'tsfeatures'. For full feature list, see linked github code repository: [[will link here]]. Time was set to the first time for that window. In the labelled data, "Activity" was set to the most common label for that window. No Activity was assigned in the unlabelled data. This processed feature data was used for subsequent 
#### Hold-out Test Set
A hold-out test set was extracted from all datasets. In all but one case, test data was made up of the full data from a randomly selected 30% of individuals from that dataset. This method of testing on the full dataset of individuals not included in the training dataset is known to be the most robust against model overfitting (Ferdinandy et al., 2020; Wilson et al., 2025). In the case of Sparkes_Koala dataset, there was insufficient labelled data to split by individual and thus, data was split chronologically with the first 80% of chronologically collected data from each ID and Activity being selected as trainign data with the remaining 20% forming the test data. While this form of model testing does not test against overfitting to the specific labelled individuals, it is robust to over-fitting to the specific behavioural events (Aulsebrook et al., 2024; Wilson et al., 2025).
#### Model Tuning and Training
The remainder of the non-test data was used to tune and train the behavioural classification model. The base behavioural classification model architecture was the Random Forest from the 'ranger' package. Tuning involves adjusting the hyperpareters of the model (parameters controliing model shape and learning ability that are set prior to learning from the data) to customise it to the data and problem. The Random Forest model has hyperparameters for the number of trees in the forest (number.trees, trialled 10-1000), the maximum number of variables to be considered at each node (mtry, trialled 3-20), and the depth of the trees (max.depth, trialled 10-50). The optimal value of these hyperparameters was tuned using the 'rBayesianOptimisation' package. 

For each set of possible hyperparameters, the model was trained and tested three times using individual-based bootstrapping such that 80% of individual's datasets were used as training data and the remaining 20% were used as the validation set, but for each repetition the individuals in each set were randomised. Due to class imbalance, models were weighted by the prevalence of each class the training data. Performance was measured as the average weighted F1 score between the three replicates where weighted F1 score was measured as the average F1 score of all individual classes in the model, weighted by their prevalence in the validation data. The hyperparameter set with the highest overall F1 score was selected as the optimal model for that dataset (==Table S1 for Optimal Model Variables... will have a supplementary materials or appendix section==). These optimal hyperparameters were used to train a final model including all non-test data.
#### Model Prediction
This final model was then used to predict behavioural classes onto the hold-out test data. Each prediction was expressed as the probability that the event belonged to each of the possible classes, and the "activity" prediction was selected as the class with the greatest probability. For datasets where unlabelled data was available, the trained model was also applied to these datasets. Additionally, the models were applied back to the training data to provide a "training example" of error in the model (see below for further description).
### Post-Processing
#### Applying Smoothing Algorithms
To determine the effect of post-processing on the classification output of the models, seven potential post-processing methods were compared to the control (no post-processing) predictions. These methods were as follows in Table 2.

***Table 2.** Summarisation of seven post-processing methods trialled in this analysis.*

| Method         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Package |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- |
| None (Control) | No post-processing was applied.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |         |
| Mode           | A rolling window of five consecutive predictions was applied. The central prediction in each window was replaced with the modal (most frequent) behaviour within the rolling window.                                                                                                                                                                                                                                                                                                                                                               |         |
| Duration       | The 95th percentile of the minimum observed duration for each behaviour was calculated from the ground-truthed training data. In the test data, any predicted sequence shorter than this threshold was coerced to the preceding behaviour to eliminate implausibly brief events.                                                                                                                                                                                                                                                                   |         |
| Confusion      | A confusion matrix was derived from the predictions on the training set, finding the probability of misclassification between each pair of ground-truth vs predicted behaviours. These probabilities were used to inform a post-processing smoother. In the test data, at every predicted transition, if the likelihood of the new class being a misclassification of the preceding class exceeded 50% (according to the matrix learnt from the training data), the transition was denied and the prediction was reassigned to the previous class. |         |
| Transition     | A transition probability matrix was calculated from the training data, capturing the likelihood of one behaviour transitioning to another within each continuous recording sample. In the test data, transitions with less than 50% probability were considered biologically implausible and corrected by reverting the transition to the preceding behaviour.                                                                                                                                                                                     |         |
| HMM            | A Hidden Markov Model was trained on the training data with the ground-truthed labels used as the true states and the predictions as the emissions. This model was then applied to the test data predictions (emissions) to determine the "true states" as smoothed predictions within each continuous recording sample.                                                                                                                                                                                                                           | xxx     |
| Bayesian       | etc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | xxx     |
| LSTM           | etc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | xxx     |
Each method was applied to each species test data predictions to produce post-processing corrected "smoothed predictions". In datasets where unlabelled data was available, these predictions were also post-processed with each of the possible methods to produce smoothed predictions.
#### Performance of Smoothing Algorithms
For each post-processing method, performance was assessed in two ways. Firstly, performance was calculated in terms of improvement to classification performance by calculating both class-specific as well as macro-averaged Accuracy, Precision, Recall, and prevalence weighted F1-Score. The model with the greatest increase in weighted F1 compared to the control (no smoothing) was considered the optimal method.

Secondly, the impact of these post-processing changes on the overall ecological interpretations was assessed using the smoothed predictions on the unlabelled data, where available. Two simple ecological questions were devised for each dataset, and the same ecological analysis conducted on each of post-processed predictions to determine how the ecological outcome was affected by the smoothing that had been applied. As there are no ground-truthed labels for these data, it is not possible to determine which of the results is more accurate, and due to the subjectivity of the ecological interpretation, nor is it intrinsically evident which would be "better". Instead, case-by-case ecologist interpretation is required to make the final decision on the robustness and usefulness of the ecological results.

For each dataset, a sequence question was designed to calculate the average duration of a bout of a particular target behaviour where a 'bout' was defined as a continuous sequence of the same behaviour. Additionally, a proportion question was designed to calculate the total proportion of an individual's day spend in the target behaviour. In the each of the four datasets with available unlabelled data, the target behaviour selected was "Walking" The questions were therefore to determine the average duration of a walking bout, and the total proportion of the day spent walking.
#### Comparison of Methods
Finally, to determine which of the methods was optimal for post-processing accelerometer-based behavioural predictions, the difference between the weighted-average F1-score for each post-processing method and the score for the control method was calculated as the 'relative improvement'. A mixed effects model was constructed with relative improvement as the dependent variable and method, proportion of sequences including transitions, and average transitions per sequence as predictor variables, and species as a random effect.
