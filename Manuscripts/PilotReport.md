# Refining Machine Learning Classification Outputs With Sequence-Informed Post-Processing: Example with Accelerometer-based Animal Behaviour Prediction

## Introduction
Machine learning models can be trained to detect specific fine-scale behaviours from animal-borne accelerometers. Despite growing sophistication in model design, however, even the best behavioural models face some fundamental constraints. Gaps in field data mean some behaviours cannot be included, while others (due to rarity, context dependence, or high variability) are difficult to classify with confidence, and performance nearly always declines across individuals and new deployment environments. Thus, while the majority of methods research has focused on improving model performance through algorithm tuning, additional sensor axes, or novel feature sets, it is improbable that perfect classification will ever be achieved. Given the inevitability of imperfect classification, therefore, ecologists are well-served considering techniques for managing and interpreting this residual error.

In the current accepted accelerometer analysis methods, the classified behavioural labels are treated as definitive categories, rather than the probabilistic predictions they actually are. In other areas of machine learning, it is standard practice to manage this uncertainty through post-processing. Post-processing is all forms of secondary inference that refines model prediction. Typically this involves applying contextual knowledge and domain-specific constraints via additional models or rule-based logic to adjust or smooth the machine learning outputs, allowing initial misclassifications to be corrected. This approach is routinely implemented across a range of fields. In speech recognition, for example, grammar rules are used to flag and correct implausible word sequences; in computer vision, unlikely object detections are filtered based on environment context; and in medical diagnostics, predictions are aligned with known symptomâ€“disease relationships to limit implausible and misleading diagnoses. In these cases, the ML model prediction is seen as only one step in a broader interpretive process.

In contrast, post-processing remains largely absent from animal accelerometry pipelines. Few studies attempt to correct or refine model outputs after prediction, and even fewer incorporate ecological knowledge into this stage. Ecologists possess domain-specific knowledge specific to the context and study species that could be used to inform such post-processing and ensure ecological reliability. For example, knowledge about the realistic minimum duration of a behaviour could allow implausibly brief events to be flagged and corrected, while a knowledge of logically acceptable behavioural sequences (walk, run) could be used to eliminate implausible sequences (lay, run). By integrating this knowledge into post-prediction workflows, we could correct misclassifications, fill gaps, and improve the ecological plausibility of behavioural classifications without needing perfect accuracy from the classification model itself.

![[post_processing_idea.png|550]] 
***Figure 1.** Post-processing. Models have known error (calculated by comparing to ground-truth classes). The predictions from models onto unknown data will have this known error, as well as additional unknown error. We can use the known error information to create a secondary smoother (post-processor) that can eliminate the known error from the predictions. While this will not remove the unknown error, it is nevertheless a reduction in error.*

In this paper, we argue that post-processing represents a critical and underutilised stage in the interpretation of accelerometer-derived behavioural classifications with potential as a pragmatic and simple path to increase interpretability of behaviour predictions while decreasing the burden to obtain perfect prediction in the ML stage. We hypothesise that post-processing can improve the consistency and ecological validity of behaviour predictions.
## Methods
For this pilot study, a single dataset was used. Data was sourced from Sparkes et al., unpublished, a koala dataset containing labelled data from 4 individual koalas containing 14 distinct behaviours. Training data contained ground-truthed behavioural labels while the test data contained both the ground-truthed classes as well as predicted classes produced by a simple machine learning model in previous research (this is stuff I did ages ago and will be a separate paper). 

Seven post-processing methods were compared to the control (no post-processing) predictions.

| Method         | Description                                                                                                                                                                                                                                                                                                              |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| None (Control) | No post-processing was applied.                                                                                                                                                                                                                                                                                          |
| Mode           | A rolling window of five consecutive predictions was applied. The central prediction in each window was replaced with the modal (most frequent) behaviour within the window, smoothing out local misclassifications.                                                                                                     |
| Duration       | The 95th percentile of the minimum observed duration for each behaviour was calculated from the ground-truthed training data. In the test data, any predicted sequence shorter than this threshold was coerced to the preceding behaviour to eliminate implausibly brief events.                                         |
| Confusion      | A confusion matrix was derived from the test set to estimate the probability of misclassification between each pair of behaviours. At every predicted transition, if the likelihood of the new class being a misclassification of the preceding class exceeded 50%, the prediction was reassigned to the previous class. |
| Transition     | A transition probability matrix was calculated from the training data, capturing the likelihood of one behaviour transitioning to another. In the test data, transitions with less than 50% probability were considered biologically implausible and corrected by reverting the transition to the preceding behaviour.   |
| HMM            | A Hidden Markov Model was trained on the ground-truthed training data and used to post-process the predicted behavioural sequences, correcting local misclassifications based on the most likely underlying state sequence inferred by the model.                                                                        |
| Bayesian       |                                                                                                                                                                                                                                                                                                                          |
| LSTM           |                                                                                                                                                                                                                                                                                                                          |
For each method, performance was assessed in two ways. Firstly, performance was calculated in terms of improvement to classification performance by calculating both class-specific as well as macro-averaged Accuracy, Precision, Recall, and F1-Score. The model with the greatest increase in macro-average F1 over the Control was considered the optimal method.

Secondly, the impact of these post-processing changes on the overall ecological interpretations was assessed. The deployment data from each species (that is, the unknown, unlabelled data) was predicted onto using the original machine learning classification model and post-processed with each of the trialled methods. Two simple ecological questions were devised for each dataset, and the same ecological analysis conducted on each of post-processed predictions. As there are no ground-truthed labels for these data, it is not possible to determine which of the results is more accurate, and due to the subjectivity of the ecological interpretation, nor is it intrinsically evident which would be "better". Instead, case-by-case ecologist interpretation is required to make the final decision on the robustness and usefulness of the ecological results.

For each dataset, a sequence question was designed to calculate the average duration of a bout of a particular target behaviour where a 'bout' was defined as a continuous sequence of the same behaviour. Additionally, a proportion question was designed to calculate the total proportion of an individual's day spend in the target behaviour.

| Species | Behaviour | Sequence question                                  | Proportion question                                 |
| ------- | --------- | -------------------------------------------------- | --------------------------------------------------- |
| Koala   | Walking   | What is the average duration of a ground-traverse? | What proportion of the total time is spent walking? |
## Results

Without post-processing, the model designed for the koala data performed with an F1 of 0.707. Bayesian Smoothing and Mode-based smoothing both present improvements over the control with F1 scores of 0.748 and 0.751 respectively. The next best method is Confusion-informed smoothing with an F1 of 0.707 (not showing on graph for some reason). These results are shown in the following graph.
![[Pasted image 20250619200826.png|500]]
The ecological question asked was what was the average frequency and duration of walking events per night in the first free-roaming individual. Without any smoothing, the average frequency was 45 and duration was 3 seconds. Between methods, however, there was a large amount of variation... see graphs.

![[Pasted image 20250619210043.png]]

## Discussion

In this pilot project, I compared 7 post-processing techniques of varying complexity to the control predictions from a koala behaviour classification dataset. Mode-based and Bayesian smoothing both improved performance over the control (no smoothing) method demonstrating the utility of these methods. 

Additionally, depending on the post-processing smoothing method implemented, the ecological results varied. While, without ground-truthed labels against which to compare, we cannot definitively state which of the methods leads to a "better" interpretation of the ecology of the species, it is interesting... (will be able to say more when I've done more data).

By incorporating higher-level inference, and utilising the sequential information available in time-series data, we are able to achieve higher classification performance without needing perfect classification accuracy during the ML prediction stage. In light of this performance gain, as a research community, we must now ask whether the substantial effort required to marginally improve classification model performance is still necessary, particularly given that simpler post-processing steps enable similar gains. Enhancing a model often demands significant resources such as collecting new data from rare behaviours or additional individuals, computationally expensive  hyper-parameter tuning, and careful balancing of model power against over-fitting. In contrast, post-processing methods can be readily applied to existing model outputs, using the data already available. 

Furthermore, in many ecological studies, the objective is not to capture behaviour at the finest temporal resolution anyway, but to reveal broader patterns of activity. These coarse-scale trends are typically robust to local misclassifications and may not require high-fidelity predictions at every time step, so long as the outputs reflect meaningful ecological processes. Thus, given that perfection is neither possible nor necessary, a more parsimonious, easily implemented, and interpreted result is suggested in post-processing.

A limitation for all post-processing methods that learn from natural sequences in the training data however is that they rely on those natural sequences being represented in the training data. That is, these methods will only present improvements when the training data represents long continuous stretches of behaviour, containing multiple natural transitions. While some studies collect behaviour in this way, it is not true of all studies. In the koala data, for example, training data was not collected in sequence but as isolated representative snippets. More than 90% of sequences in the training data contained only 1 behaviour, with 2 behaviours in only very small minority. Thus, there were very few transitions for the models to learn from, providing such limited information that the higher-order post-processing methods failed to confer advantages over the most basic modal smoothing. This may explain why the HMM and LSTM methods failed to perform in the koala pilot data.
## Conclusion

Results are promising and I should continue onto a full-scale investigation.
## Where to from here?
- 5 species (or rather, however many good datasets I can get a hold of)
	- This may present a problem if I'm unable to find naturalistic training data (i.e., with the sequences maintained... in which case I am generating my own cat data with the help of some research assistants)
- Get back in contact with Dr Hui Yu from Deakin to consider getting his collaboration on this chapter? He and I have already discussed coming up with some kind of project to collaborate on. He did mode based smoothing in some of his papers. It would be good for me to make collaborative connections.
- NOTE TO SELF: Adjust the manual methods to not learn from false transitions (i.e., update them to match the HMM, Bayes, and LSTM rules)
## Target Journals
- Phil Tran B??
- Running out of journals that are a) free/cheap, and b) intersect methods and eco lol